{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5PAHPPSY: Big Data Analytics in Python: Session 3\n",
    "\n",
    "___\n",
    "\n",
    "_Jodie Lord<br/>\n",
    "Department of Basic and Clinical Neuroscience<br/>\n",
    "Institute of Psychiatry, Psychology and Neuroscience<br/>\n",
    "King's College London<br/>\n",
    "jodie.lord@kcl.ac.uk_\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üòç Today's Learning Objectives:\n",
    "<br/>\n",
    "\n",
    "1. Explore the world of charting in python via two key charting libraries:\n",
    "    - MatPlotLib üìä\n",
    "    - seaborn üê¨\n",
    "\n",
    "\n",
    "2. Learn how to carry out some basic statistics üìà\n",
    "\n",
    "\n",
    "3. Combine learning from charting libraries and statistics to produce meaningful visualisations for statistics produced. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã BUT FIRST - ITS TIME TO RECAP AGAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>‚å® Recap Exercises </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R1: Loops üîÉ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of variables from 10:100 in steps of 10. Assign this list to the object \"tens\"\n",
    "tens=\n",
    "\n",
    "\n",
    "#print this list of items onto the screen using a for loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Using a loop, assign the variables within the \"tens\" list to the \"under50\" list generated for you below, but ONLY \n",
    "if the variable is equal to or less than 50 (this should result in an under50 list of 5 variables).\n",
    "Hint1: You wil need to use an if statement here\n",
    "Hint2: Make use of .append() to add items to an existing list (google if unsure!)\"\"\"\n",
    "\n",
    "# Initiating empty list\n",
    "under50=[]\n",
    "\n",
    "# Running conditional loop\n",
    "            \n",
    "\n",
    "# Print the under50 to the console:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R2: pandas üêº**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the cognitive_scores.csv file and assign to the object \"df1\" \n",
    "# Hint1: Make sure you've imported your necessary libraries!\n",
    "# Hint2: If python doesn't recognise your csv file: 1) have you spelt the filename correctly?! 2) Are you pointing to the correct directory?\n",
    "\n",
    "\n",
    "# Load in brain_size_clindata.csv\n",
    "\n",
    "\n",
    "# Print the head of the df into the console:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the second csv file: \"clinical_scores.csv\" and assign it to the object \"df2\"\n",
    "df2=\n",
    "\n",
    "# Print rows 10:20 of this second df into the console (remember indexing starts at 0!):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R3: Working with data üõ†**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df1 and df2 together using an inner join\n",
    "\n",
    "\n",
    "#View the tail of the newly merged df:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm shape of df1\n",
    "\n",
    "\n",
    "# Confirm shape of df2\n",
    "\n",
    "\n",
    "#Confirm shape of df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have any nas in our merged dataset? - check using isna() and the sum() command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df object called df_nonulls which is df_all but with all nas dropped:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the number of participants we have in the df_nonulls dataset (using len):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the mean of the Height column in df_nonulls, grouped by gender\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "\n",
    "## (1) Charting Libraries üìäüê¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Last week we learnt how to explore our data using some key descriptives and interrogating our data output. \n",
    "\n",
    "\n",
    "* Another important step of data exploration is through the use of charting and visualisation. This allows you to quickly eyeball trends within the data, easily spot seperation within the data (e.g. between two groups), or identify potential problems such as outliers.\n",
    "\n",
    "\n",
    "* Today we will be using two charting libraries available within python to visualise our data:\n",
    "\n",
    "    + **matplotlib** üìä - the standard / legacy plotting library within python (see: https://matplotlib.org/)\n",
    "    + **seaborn** üê¨ - a library built on top of matplotlib which helps to make your plots look pretty üå∏ (see: https://seaborn.pydata.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Accessing charting libraries**\n",
    "\n",
    "* As with all libraries, to make use of both matplotlib and seaborn, we first need to import them in: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing matplotlib and assigning to name \"plt\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing seaborn and assigning to name \"sns\"\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* üîä **NOTE**: When working within Jupyter Notebook, it is also important to **always** use the below magic function when you import matplotlib, otherwise you won't be able to see your plots! üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allowing plots to print inline with notebook:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploring distributions using histograms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One assumption of many standard \"parametric tests\" in data analytics is that continuous data is normally distributed. \n",
    "\n",
    "* Histograms are a quick way to check whether assumptions of normality are met, and can be plotted easily using python charting libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>‚å® Tasks </font>\n",
    "* Run the below code to check whether scores on cognitive test 2 are normally distributed in our df_nonulls dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running \"dist\" plot in seaborn (\"dist\" = distribution). \n",
    "#Telling python to use seaborn by prefixing it with sns which we assigned seaborn as when importing above\n",
    "sns.distplot(df_nonulls['CogTest2'], kde=False, bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can edit and format the output of charts py passing parameters within your plot parentheses. For example, re-run the above, but with the kde parameter changed to kde=True. \n",
    "    + What does this do to your plot? \n",
    "    + Why might this be useful? ü§î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-running hist with kde=True \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now display the distribution of **weight** for **female participants only** - are we confident that this data is normally distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Histogram for female weight distributions\n",
    "sns.distplot(df_nonulls[\"Weight\"][df_nonulls['Gender'] == 'Female'], kde=True, bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö **_More information on plotting histograms, and histogram options within seaborn can be found [here](https://seaborn.pydata.org/tutorial/distributions.html)._**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparing group means using barcharts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bar charts are an effective basic visualistion for eyeballing whether the means between two groups are different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>‚å® Tasks </font>\n",
    "* Run the below code to check whether mean scores on `CogTest1` look different between males and females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running barplot in seaborn (specified by sns), with gender on x axis, and CogTest1 on y axis:\n",
    "sns.barplot(data=df_nonulls , x=\"Gender\", y=\"CogTest1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the web / seaborn documentation, look up what the `hue` parameter does.\n",
    "* Add the `hue` parameter to your `barplot` to see differences between diagnosed and non-diagnosed males and females - any evidence of a difference?\n",
    "* Additionally, change the colour palette of the plot to \"Blues\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete the code below, adding hue:\n",
    "sns.barplot(data=df_nonulls , )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö_**More information on plotting bargraphs, and options within seaborn can be found [here](https://seaborn.pydata.org/generated/seaborn.barplot.html).**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visalising joint relationships using scatterplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scatter plots can be used to visualise the joint relationship between two continuous variables.\n",
    "* Seaborn offers a number of options for visualising data in a scatter format, including a simple `scatterplot` or `relplot` which allows you to view the `rel`ationship between two variables, as well as the `lmplot` which allows you to plot a linear line of best fit through your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>‚å® Tasks </font>\n",
    "* Run the below code see whether there is any indication of a joint relationship between scores on cognitive test 1 and weight in the df_nonulls dataset, using `relplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating seaborn scatterplot using relplot (\"rel\" = \"relationship\"), with Cogtest1 on x axis and Weight on y axis\n",
    "sns.relplot(x=\"CogTest1\", y=\"Weight\",data=df_nonulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "* Now look at the relationship between height and weight, this time using the `lmplot` visualisation.\n",
    "    + Is there any evidence of a relationship between these two variables?\n",
    "    + What is noticeably different about this plot visalisation compared to the one we previously ran with `relplot`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating scatter using replot, to plot weigt (x-axis) against height (y-axis)\n",
    "sns.lmplot(x=\"Weight\", y=\"Height\",data=df_nonulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As with the other plots, you are able to add extra parameters to scatter visualisations to change formatting or to provide you with extra information.\n",
    "* What happens when you add hue=\"Diagnosed\" to the height/weight scatter plot?\n",
    "* What about hue=\"Gender\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-generating height/weight scatter, with hue=Diagnosed\n",
    "sns.lmplot(x=\"Weight\", y=\"Height\",data=df_nonulls, hue=\"Diagnosed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Complete the code below so that it re-runs the same code as above but change hue=Diagnosed to Gender\n",
    "sns.lmplot("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö _**More information on scatter plots in seaborn can be found [here](https://seaborn.pydata.org/generated/seaborn.relplot.html) and [here](https://seaborn.pydata.org/generated/seaborn.lmplot.html)**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîä Seaborn has a whole host of charting visualisations and parameter options within each of these. We are only scratching the surface here - spend some time familiarising yourself with some of the [seaborn documentation](https://seaborn.pydata.org/index.html) - this will come in handy during your assessment... üëÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíæ Saving figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Charting visualisations can be saved using the `savefig()` argument. To do this you must:\n",
    "    + First assign your chart an object name.\n",
    "    + Confirm the pathname, filename, and file extension when saving your file (e.g. `/path/to/file/filename.extension`). An example of an extension may be a `.png` or a `.jpeg`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>‚å® Task </font>\n",
    "* Run the below code to generate a barplot visualisation and to save this within your current working directory (so no need to specify `/path/to/file`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating plot and assigning it to variable 'g'\n",
    "\n",
    "g=sns.lmplot(x=\"CogTest3\", y=\"CogTest1\",data=df_nonulls, col=\"APOE4\", hue=\"Gender\") #Notice the addition of the 'col' parameter - what do you think this does?\n",
    "\n",
    "# Saving the plot within current directory by specifying file name and file extension (.png in this case)\n",
    "\n",
    "g.savefig(\"myFirstPythonFigure.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* !! **Check your directory** !! Is it there? Can you open it? Does it show what you want?\n",
    "\n",
    "\n",
    "* **Now**:\n",
    "    + Generate a barplot looking at `Diagnosed` (x axis) against `CogTest1` (y axis).\n",
    "    + Format the chart so that it is coloured with the \"purples\" palette.\n",
    "    + Add a legend for gender\n",
    "    + Assign the chart to an object named `c`.\n",
    "    + Save the chart within your current directory named \"MySecondPythonFigure\", and as a jpeg image.\n",
    "\n",
    "\n",
    "**Note:** Sometimes (with certain chart types - not sure why ü§∑) python won't let you use `savefig` option to save your figure unless you place a `figure` argument before it (e.g. `g.figure.savefig()`). Be aware of this and try adding it if you recieve an error when attempting to save your visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plot and assigning it to variable 'c':\n",
    "\n",
    "\n",
    "# Save the plot with as MySecondPythonFigure with .jpeg extension:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "\n",
    "### (2) Data Analytics üìà"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plotting raw data can be useful to get a general feel for you data and indetify any obvious issues which need to be handled. However, visualising data alone can't quantify any degree of confidence with which any differences or patterns that you may be seeing are meaningful (or rather, statistically significant). To do this, we need to perform some key data analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading stats library**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As with anything in python, before we can run any of our basic statistics, we will need to import neccessary libraries which allow us to do this.\n",
    "* Today we will work with two two stats libraries: `scipy` and `statsmodels`. Load these below üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the scipy library and assigning it the name 'stats'\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Importing the statsmodels library and assigning it the name 'sm'\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols #This imports a particular stats method we will be using later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing assumptions of normality with Skewness, Kurtosis, and Shapiro-Wilks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We previously made use of seaborn's histogram charting component to get a feel for how \"normal\" the distribution of our data looked.\n",
    "\n",
    "\n",
    "* Data that does not follow a nice bell üîî shaped distribution risks violating our assumptions of normality. \n",
    "\n",
    "\n",
    "* Histograms are a good starting point to detect any obvious distribution issues, but it is also useful to confirm and quantify these assumptions using statistics.\n",
    "\n",
    "\n",
    "* Python's `scipy` package allows you to quickly return normality stats using built-in functions such as `shapiro`, `skew`, and `kurtosis`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skewness and kurtosis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=midnightblue>A reminder of what skewness and kurtosis visually look like:</font>**\n",
    "\n",
    "\n",
    "![](skew_and_kurtosis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When looking at the numbers, we'll assume:\n",
    "\n",
    "    + Anything ¬±0.5 is moderately skewed, and anything ¬±0.5 is highly skewed.\n",
    "    \n",
    "    + Anything ¬±4.0 is evidence of kurtosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>‚å® Tasks </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the code chunks below to confirm whether there is any notable skewness or kurtosis within `CogTest2` results.\n",
    "\n",
    "\n",
    "* Does this align with what we previously visualised with our histograms?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating skewness in CogTest2:\n",
    "stats.skew(df_nonulls[\"CogTest2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating kurtosis in CogTest2:\n",
    "stats.kurtosis(df_nonulls[\"CogTest2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now run the same tests against the `Weight` measure within our `df_nonulls` dataset.\n",
    "\n",
    "* Any evidence of skewness/kurtosis? Does this align with the assumptions we made from our histogram visualisations?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating skewness in Weight:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating kurtosis in Weight:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, run `skew` and `kurtosis` metrics against the `MusicComp` results.\n",
    "\n",
    "* Any evidence of skewness/kurtosis?\n",
    "\n",
    "* Plot a histogram using the seaborn charting component (`distplot`) to see whether the stats are in agreement with the visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating skewness in MusicComp:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating kurtosis in MusicComp:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histogram to visualise distribution:\n",
    "sns.distplot(df_nonulls[\"MusicComp\"], kde=False, bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "**Shapiro-wilks test of normality**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another useful test to (dis)confirm normality assumptions is the shapiro-wilks test - available in `scipy` using the `shapiro` function. \n",
    "\n",
    "\n",
    "* The **null hypothesis** for this test is that the data are **normally distributed**. Thus, for assumptions of normality to be met using this statistic, we want to see a **non-significant p-value**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>‚å® Tasks </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the code below to perform a shapiro-wilk test on our `CogTest2` results. \n",
    "\n",
    "\n",
    "* Two numbers are returned within the output. What do each of these refer to (information can be found [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html))?\n",
    "\n",
    "\n",
    "* Do results suggest that normality assumptions are met for this cognitive measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running shapiro test available within the scipy (\"stats\") library against CogTest2:\n",
    "stats.shapiro(df_nonulls[\"CogTest2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* Use the same code to test normality assumptions for our `weight` variable.\n",
    "\n",
    "\n",
    "* Do results suggest noramlity assumptions are met for this measure?\n",
    "\n",
    "\n",
    "* Does this result match the assumptions we made when looking at our histogram? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running shapiro-wilks against weight:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* What about when you test the distribution for females only?\n",
    "\n",
    "* And men only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing shapiro-wilks for weight, filtered so female weight scores only\n",
    "stats.shapiro(df_nonulls[\"Weight\"][df_nonulls['Gender'] == 'Female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing shapiro-wilks for weight, filtered so male weight scores only\n",
    "stats.shapiro(df_nonulls[\"Weight\"][df_nonulls['Gender'] == 'Male'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>üí¨ Discussion </font>\n",
    "\n",
    "* How may we explain what is going on here? ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "### Comparing differences between **two groups** using a t-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=midnightblue>A reminder</font>**\n",
    "\n",
    "* T-tests are used when you have **2 groups** and the dependent measure is **continuous**.\n",
    "\n",
    "\n",
    "* It compares the mean result from group 1 against the mean of group 2.\n",
    "\n",
    "\n",
    "* There are various types of t-tests (today we will just perform the first of these):\n",
    "    + **Independent samples test** which measures mean differences assuming results from group 1 and group 2 are from different individuals\n",
    "    + **Paired samples test** which assumes results from group 1 and 2 are matched (e.g. in a repeated measures design).\n",
    "    + **One sample test** which tests the results from a single group against a known mean.\n",
    "\n",
    "\n",
    "* The t-test returns a \"t-statistic\" which has a corresponding p-value as an indication of whether differences in the means are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**T-test within the [scipy library](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html)**\n",
    "\n",
    "* We can use the `ttest` function within the `scipy` library to return a t-statistic and corresponding p-value for a comparison between two sets of means.\n",
    "\n",
    "\n",
    "* The `_ind` option can also be used to confirm that we are looking for an **independent** samples test.\n",
    "\n",
    "\n",
    "* _**Note**_ The t-test is a parametric test and so works according to our normality assumption. We have already confirmed that this assumption is met for scores within our `CogTest2` metric, so we can utilise this measure to run our test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>‚å® Tasks </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that our `Diagnosed` metric corresponds to whether an individual has recieved a diagnosis of dementia (1) or has recieved no such diagnosis (0)...\n",
    "\n",
    "\n",
    "* Run the below code to test the following hypotheses:\n",
    "    + _H1_: There is a signficant difference in cognitive performance between individuals diagnosed with dementia compared to those not diagnosed.\n",
    "    + _H0_: There is no signficant difference in cognitive performance between individuals diagnosed with dementia compared to those not diagnosed.\n",
    "    \n",
    "    \n",
    "* Based on the t-statistic and pvalue returned, which hypothesis should we accept, and which should we reject?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running independent samples t-test of cogtest2 scores against non-diagnosed (0) and diagnosed (1) individuals:\n",
    "stats.ttest_ind((df_nonulls[\"CogTest2\"][df_nonulls['Diagnosed'] ==0]), \n",
    "                (df_nonulls[\"CogTest2\"][df_nonulls['Diagnosed'] ==1])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run a second t-test, but this time looking at differences in `CogTest3` scores between males and females:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run independent samples t-test of cogtest3 scores against females and males:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>üí¨ Discussion </font>\n",
    "* Is there any evidence of a significant difference in cognitive test scores for this measure between males and females?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "### Comparing differences between three of more groups using ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=midnightblue>A reminder</font>**\n",
    "\n",
    "* ANOVAs are used when you have **3 or more groups** and when dependent measures are **continuous**.\n",
    "\n",
    "\n",
    "* It is similar to the t-test, but compares the means across all groups to determine whether there is an overall signficant difference in means across groups.\n",
    "\n",
    "\n",
    "* In an ANOVA - the test statistic is an F (rather than t), and the p-value provides an indication of whether differences in the means across groups are significant.\n",
    "\n",
    "* Like T-tests - they assume normally distributed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANOVA within the [scipy library](https://pythonfordatascience.org/anova-python/)**\n",
    "\n",
    "* We can use the `f` function within the `scipy` library to return an f-statistic and corresponding p-value for a comparison between 3 or more means.\n",
    "\n",
    "\n",
    "* The `_oneway` option can also be used to confirm that we are looking for a **oneway** ANOVA (i.e. we're interested in one independent variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>‚å® Tasks </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background:**\n",
    "* APOE4 is a genetic risk factor that is highly associatde with Alzheimer's Disease (a form of dementia). \n",
    "* All individuals have two copies of their genes (1 copy inherited from their mum, and one from their dad).\n",
    "* Therefore, an individual may have 1 copy of the APOE4 gene, 2 copies, or 0 copies. \n",
    "* Past research indicates that there is a \"dosage\" effect of APOE4 on cognition and dementia risk.\n",
    "\n",
    "\n",
    "* Run the below code to test the hypothesis that differences in APOE4 \"dosage\" (i.e. whether you have 0,1 or 2 copies) is signficantly associated with cognitive test results, using our CogTest2 scores which we have already confirmed meet our normality assumptions.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.f_oneway(df_nonulls['CogTest2'][df_nonulls['APOE4'] == 0], \n",
    "             df_nonulls['CogTest2'][df_nonulls['APOE4'] == 1],\n",
    "             df_nonulls['CogTest2'][df_nonulls['APOE4'] == 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>üí¨ Discussion </font>\n",
    "\n",
    "* Based on the t-statistic and pvalue returned, is there support for our hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**(optional) Homework**\n",
    "\n",
    "* The `scypi` results give us a quick answer to our hypothesis. The results are all rather vague though! üò© It's great that we can confirm an f-stat and p-value, but that doesn't really provide us with a rich source of information to interpret...\n",
    "\n",
    "\n",
    "* The `statsmodels` library that we also imported it earlier can be a handy additional tool in your toolbox for if you want to gain a richer insight into your results.\n",
    "\n",
    "\n",
    "* The `ols` function within the `statsmodels` library can be used to gather ANOVA statistics (note: OLS is actually a regression method, but within it we can obtain the stats (and more!) that we need.\n",
    "\n",
    "\n",
    "* More information about ANOVAs in the `statsmodels` package can be found [here](https://pythonfordatascience.org/anova-python/). Explore it in your own time and see what other information you may be able to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "### Investigating the strength and direction of relationship using Pearsons Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=midnightblue>A reminder</font>**\n",
    "\n",
    "* A pearson's correlation is used to compare the direction and strength of **two continuous variables**.\n",
    "\n",
    "\n",
    "* The correlation coefficient is represented as `r`.\n",
    "\n",
    "\n",
    "* r can fall anywhere between -1 and +1. \n",
    "\n",
    "    + An r of 0 indicates no relationship between 2 variables at all.\n",
    "\n",
    "    + An r of -1 indicates a perfect negative correlation (as 1 variable goes up, the other variable goes down, at exactly the same rate).\n",
    "\n",
    "    + An r of +1 indicates a perfect positive correlation (as 1 variable goes up, the other variable also goes up, at exactly the same rate).\n",
    "\n",
    "    + The higher the number towards 1 - the greater the strength of relationship. + vs - represents directionality.\n",
    "\n",
    "\n",
    "* You **cannot** infer causality from a correlation - it provides no information to which variable is driving the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pearson's correlation within pandas üêº**\n",
    "\n",
    "\n",
    "* Pandas actually comes with the ability to return correlation coefficients - so no need for external stats packages here (more information [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html)).\n",
    "\n",
    "\n",
    "* Calling the `.corr()` function within our `pd` library will create a correlation matrix for you and gives you the correlation coefficiant.\n",
    "\n",
    "\n",
    "* You can additionally style the correlation matrix returned using the `style.background_gradient` function to apply overlay a heatmap onto the coefficients returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>‚å® Tasks </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the code below to produce a correlation matrix for all of your continous cognitive measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producing correlation coefficients\n",
    "df_nonulls[[\"CogTest1\", \"CogTest2\", \"CogTest3\", \"SpeedComp\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Overlay results within a heatmap using the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding heatmap:\n",
    "df_nonulls[[\"CogTest1\", \"CogTest2\", \"CogTest3\", \"SpeedComp\"]].corr().style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>üí¨ Discussion </font>\n",
    "* Which 2 measures have the strongest relationship? What is the coefficient for this?\n",
    "* Is the direction of effect between CogTest1 and SpeedComp positive or negative? How would you interpret this?\n",
    "* Overall - would you say these measures are well correlated with each other? Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Height and weight**\n",
    "* Confirm pearsons r for height against weight.\n",
    "\n",
    "    + Is there evidence of a strong relationship between the two?\n",
    "    + Is the direction of relationship positive or negative?\n",
    "    + Is this in agreement with what we saw on our scatter plot earlier today?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce correlation coefficients for height against weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_psssstüëÄ_  \n",
    "* The pandas correlation matrix doesn't allow you to return a p-value against your r statistic unfortunately. \n",
    "* However, you can use the `pearsonr` function within the `scipy` library to get this information if required (see [here](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html)).\n",
    "* Caveat with this is that you can only return **one** coefficient and p-val at a time, rather than spit out results into an overall matrix to compare a set of results (there are functions you can write to achieve this, but let's not overly complicate things on day 3, just know it's possible!).\n",
    "\n",
    "\n",
    "* As an example, run the below code to obtain a p-value for the weight-height correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(df_nonulls[\"Height\"],df_nonulls[\"Weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>üí¨ Discussion </font>\n",
    "* Is there evidence of a significant relationship?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Some final learning tasks üôå**\n",
    "\n",
    "- Have a look at the documentation for the seaborn library - play around with other charting components and parameters to see what other visualisations you can produce.\n",
    "\n",
    "\n",
    "- Confirm p-values for each of the cognitive measure correlation coefficients using scipy. \n",
    "    + Any evidence of significant relationships amongst these measures?\n",
    "    + Any unexpected p-values based on the value of the coefficient?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "**(optional) Homework üìö:**\n",
    "- Take a look at some of the documentation for running regressions within the `statsmodels` library (for linear regression: [here](https://datatofish.com/statsmodels-linear-regression/). for logistic regression: [here](http://blog.yhat.com/posts/logistic-regression-and-python.html) We didn't have time to cover this method today but it will be an extremely useful tool in your toolbox if you're looking to develop your datascience skills further.\n",
    "    + Attempt to write a piece of code using logistic regression and the statsmodel library to model the relationship between CogTest2 (predictor) and Diagnosed (dependent measure). \n",
    "    + What is the regression coefficient returned? Is it positive or negative?\n",
    "    + Is the regression coefficient significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### NEXT WEEK  üëÄ\n",
    "\n",
    "+ We'll do some (basic) machine learning! üé∞\n",
    "    \n",
    "    \n",
    "+ Last 30 mins: We'll wrap up what we've learnt over the last 4 weeks. Please come with any questions! ‚ùì\n",
    "\n",
    "\n",
    "+ Overview of assessment structure and expectations üìù\n",
    "\n",
    "üôã\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
