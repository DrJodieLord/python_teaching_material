{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5PAHPPSY: Big Data Analytics in Python: Session 4\n",
    "\n",
    "___\n",
    "\n",
    "_Jodie Lord<br/>\n",
    "Department of Basic and Clinical Neuroscience<br/>\n",
    "Institute of Psychiatry, Psychology and Neuroscience<br/>\n",
    "King's College London<br/>\n",
    "jodie.lord@kcl.ac.uk_\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Last Set of üò¢) Learning Objectives:\n",
    "<br/>\n",
    "\n",
    "1. Grasp the basic concept of machine learning. \n",
    "\n",
    "\n",
    "2. Learn some (basic) machine learning techniques üé∞\n",
    "\n",
    "\n",
    "3. Consolidate what we've learnt over the last 4 weeks.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Last 15 mins:**\n",
    "\n",
    "Overview of assessment structure and expectations üìù\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úã BUT FIRST - ITS TIME TO RECAP AGAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>‚å® Recap Exercises </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R1: Data Analytics üìà**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the cog_clin_data.csv file (this is the merged dataset we created last week, saved as a .csv file) - name df\n",
    "## View the head of the file onto the screen\n",
    "## Hint: Remember your libraries!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check skewness and kurtosis of MRI_counts in the data \n",
    "## Assume skew ¬±0.5 = moderate and ¬±0.7 high\n",
    "## Assume kurtosis ¬±4.0 = high\n",
    "## Hint: Remember your libraries:\n",
    "\n",
    "#Load stats library\n",
    "\n",
    "\n",
    "#Check skewness for MRI_counts:\n",
    "\n",
    "\n",
    "#Check kurtosis for MRI_counts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run a shapiro-wilks test on the same variable - what does this show? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R2: Charting  üìäüê¨**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in your charting libraries\n",
    "\n",
    "# Import matplotlib and assigning to name \"plt\"\n",
    "\n",
    "\n",
    "# Import seaborn and assigning to name \"sns\"\n",
    "\n",
    "\n",
    "\"\"\"You'll need to also perform an additional step to ensure you can see your charts - \n",
    "can you remember what this is?\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the seaborn library to plot a histogram of the MRI_Count data \n",
    "## Is the visualisation in agreement with what our stats tell us?\n",
    "\n",
    "#Plot histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use scatter plot functionality within seaborn to look at the relationship between MRI_count and CogTest2\n",
    "## Plot a regression line through the plot - is there evidence of a relationship between these two variables?\n",
    "\n",
    "#Plot correlation using the lmplot function so that a regression line can be fit to the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save your figure as a jpeg file within your current working directory\n",
    "## Hint: Remember to assign the chart as an object first:\n",
    "## DOUBLE CHECK YOUR FOLDER ON YOUR DESKTOP ONCE DONE - IS IT THERE????\n",
    "\n",
    "#Assign chart as object:\n",
    "\n",
    "\n",
    "#Save the figure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run a pearsons correlation using the stats package to confirm whether the stats agree with what we see here:\n",
    "## What is the r?\n",
    "## What is the p-val? - is there evidence of a significant relationship?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualise this, along with the correlation coefficient with height and weight using pandas \n",
    "## Overlay with a heatmap\n",
    "\n",
    "#Create correlation matrix using pandas:\n",
    "#Add heatmap:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "\n",
    "## (1) Introduction to Machine Learning üé∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is machine learning?..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine learning is a broad concept which refers to specialised algorithms designed to take in a (typically very large) set of variables, and use information contained within those variables to make predictions about future events.\n",
    "- As the name suggests, the algorithms \"learn\" from the data, and update their information accordingly.\n",
    "\n",
    "**The basic ideas is:**\n",
    "1. Receive a collection of observations / variables associated with an outcome of interest.\n",
    "\n",
    "2. Train a ‚ÄúMachine Learning‚Äù model using this data.\n",
    "\n",
    "3. Predict events on a new set of data relating to the same outcome of interest, using the trained algorithm.\n",
    "\n",
    "- üîç For those interested in learning more, [this article](https://www.nature.com/articles/nmeth.4642) provides a good overview of the differences between classic statistics and machine learning.\n",
    "\n",
    "**Different types of machine larning exist:**\n",
    "- Supervised            (_*the focus of today*_)\n",
    "- Unsupervised\n",
    "- Reinforcement Learning\n",
    "\n",
    "**Different types of outcomes include:**\n",
    "- Binary               (_*the focus of today*_)\n",
    "- Continous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/supervised_unsupervised.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/supervised_sunsupervised_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "## <font color=Red>  üö¢ Would YOU survive the titanic?! </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Last session - Let's have some fun with this!** üï∫\n",
    "\n",
    "\n",
    "- Here we'll use an applied (simulated) example in which we'll use variables representing charactoristics of individuals onboard the titanic when it sank to predict whether or not - in a new situation where a ship were sinking - individuals with certain charactoristics would be likely to survive.\n",
    "\n",
    "**THE BACKGROUND**\n",
    "- The 1912, the infamous Titanic ship, travelling on it's way to America, collided with an iceberg.\n",
    "- The ship began to sank and because there weren't enough lifeboats onbaord to cater for everyone, many passengers and crew did not get out alive.\n",
    "- If you've seen the film (‚ù§Ô∏èÔ∏è) you'll know that individuals with certain charactoristics were were more likely to recieve entry to a lifeboat than others - making them more likely to survive the sinking ship.\n",
    "\n",
    "**THE CHALLENGE**\n",
    "- Can we use information about those who did or did not survive to predict who would be more likely to survive the sinking ship?...\n",
    "\n",
    "\n",
    "- To do this we'll need to know:\n",
    "    - What data we have available \n",
    "    - What the most important \"features\" are which may predict our outcome (i.e. which charactoristics are most predictive of our outcome)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>‚å® Task </font>\n",
    "\n",
    "- Load in the `titanic.xls` file into your working session and assign it the name `titanic`.\n",
    "- View the head of the file.\n",
    "- Confirm how many rows and columns we have\n",
    "- What are the datatypes of the variables?\n",
    "- Any missing values?\n",
    "- How many passengers survived? How many died?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the titanic dataset\n",
    "titanic=pd.read_excel(\"titanic.xls\")\n",
    "\n",
    "## View the head of the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confirm total rows and columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confirm what kind of datatypes we have in our file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confirm whether we have any missing values in our data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Check how many passengers survived by running the below code:\n",
    "titanic['survived'].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________\n",
    "\n",
    "_N.B. Columns within the titanic data are:_\n",
    "\n",
    "- survived - did the passenger survive (0 = No; 1 = Yes)\n",
    "- pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "- name - Name\n",
    "- sex - Sex\n",
    "- age - Age\n",
    "- sibsp - Number of Siblings/Spouses Aboard\n",
    "- parch - Number of Parents/Children Aboard\n",
    "- fare - Passenger Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=Midnightblue> **Important things when working with machine learning models:** </font>\n",
    "- Free-text is not interpretable by ML models - must include numeric data types only!\n",
    "- Missing data causes problems / biases. Must be either imputed or removed.\n",
    "- Columns are interpreted as \"features\", rows are interpreted as \"observations\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>üí¨ Discussion </font>\n",
    "\n",
    "- Based on what we know about our data - which variables could cause us problems here? ü§î\n",
    "- How should we deal with these (if at all)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________\n",
    "### (1a) Preparing your data üõ†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preperation is an **extremely** important step in ensuring your machine learning models run well. We therefore need to deal with our missing data and our non-numeric variables. \n",
    "\n",
    "For today we will:\n",
    "- Impute our single missing fare value using information from our pclass feature.\n",
    "- Drop our missing age observations.\n",
    "- Recode our sex strings into numeric form.\n",
    "- Drop our name feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>üí¨ Discussion </font>\n",
    "\n",
    "- Take a look at the decisions we've made above - why do you think each of these decisions have been made?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Imputing fare using pclass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First: confirm the pclass for the missing fare observation:\n",
    "## Hint: Make note of the row index for this observation also - we will use it later!\n",
    "\n",
    "titanic.loc[titanic['fare'].isna()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Second: Confirm the mean fare paid by each class of passengers \n",
    "## What was the mean fare paid by 3rd class passengers?\n",
    "\n",
    "titanic.groupby('pclass')['fare'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Third: Define a function which imputes using the mean values of the fare column, by the categories within\n",
    "the pclass column (i.e. this lets us impute mean fair by passenger class).\n",
    "To do this, we need to define a few steps, as outlined below...\n",
    "\n",
    "WARNING: We haven't covered user defined functions within our sessions so the below may look a little scary/confusing!\n",
    "DON'T PANIC - just know that this ability exists. Knowing what to change to get the information you need is all you\n",
    "need at this point\"\"\"\n",
    "\n",
    "\"\"\"YOU ARE NOT EXPECTED TO KNOW USER DEFINED FUNCTIONS FOR THE EXAM - IT IS JUST A 'NICE TO HAVE'\"\"\"\n",
    "\n",
    "# 1. Create a groupby object: by_pclass\n",
    "by_pclass = titanic.groupby(['pclass'])  #This creates a new datatable which groups observations by passenger class\n",
    "                                         #If you wanted to impute the mean of a value based on e.g. sex (instead of pclass), you would just change this to .groupby(['sex'])\n",
    "                                         #If you wanted to impite the mean based on e.g. sex AND pclass, you would just change this to .groupby(['pclass', 'sex'])\n",
    "\n",
    "        \n",
    "# 2. Write a function that imputes mean\n",
    "def fillmissing_mean(series):\n",
    "    return series.fillna(series.mean())  #This is a user defined function. No computation is happening here - we are just defining the instructions for, and naming the function (naming it: fillmissing_mean)\n",
    "                                         #This essentially says: when this function is used (as in step 3), for the series of values specified, fill na values using the series mean. \n",
    "                                         #If you wanted to change this so it filled missing values by e.g. the median you would simply change series.mean() to series.median() (and you'd probably want to change the name of the function from fillmissing_mean to fillmissing_median - to make it clear what the function is doing!)\n",
    "\n",
    "        \n",
    "# 3. Impute age and assign to titanic.age\n",
    "titanic['fare'] = by_pclass['fare'].transform(fillmissing_mean)    #This is essentially saying:\n",
    "                                                                   #For my 'fare' column within my titanic datatable\n",
    "                                                                   #Impute (\"fill in\") missing values on the basis of which passenger class that observation belongs to\n",
    "                                                                   #Python knows we want to fill nas using the mean, becuase we have passed through out fillmissing_mean function which we defined in the step before.\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "\n",
    "üîç *A nice datacamp tutorial on filling data using aggregates etc can be found [here](https://campus.datacamp.com/courses/manipulating-dataframes-with-pandas/grouping-data?ex=1)*\n",
    "\n",
    "üîç *__For the keen__: [here](https://www.w3schools.com/python/python_functions.asp) and [here](https://www.learnpython.org/en/Functions) offer easily digestible introductions to user define functions (entirely optional but extremely useful).*\n",
    "\n",
    "___________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth: Confirm there are no missing longer any missing fare values in your data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Fifth: Confirm the value for the missing datapoint has changed to the mean fare value based on \n",
    "3rd class passengers by filtering out the row index for this missing value\"\"\"\n",
    "\n",
    "\"\"\" Hint: We filtered our table by the fare.isna() above, so you can use this to identify what the row index is for \n",
    "this missing value.(look at the first, unnamed column - this is your row index!)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Dropping missing age observations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the .dropna() function to remove all missing age observations from your data\n",
    "titanic=titanic.dropna()\n",
    "\n",
    "\n",
    "## Pssst ##\n",
    "## In this case, age is our only remaining feature with missing values, so dropna() across the entire data table is fine.\n",
    "## If had multiple features with missing data and only wanted to drop nas for age, we could use the subset= option within dropna function:\n",
    "\n",
    "#titanic=titanic.dropna(subset=['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check your data still looks sensible:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confirm you no longer have any missing values in your datatable:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Removing uneccessary / problematic features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the .drop() function to remove our uneccesarry name feature from the data:\n",
    "titanic = titanic.drop(['name'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Psssst üëÄ**\n",
    "- axis=1 is simply telling python that what we want to drop is a column variable.\n",
    "- axis=1: telling python to look across columns\n",
    "- axis=2: telling python to look across rows\n",
    "- Some functions require you to explicity state which axis you are referring to. So if python throws an error relating to an \"axis\", you likely need to add in this to your argument!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/axis.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your data - have the names been appropriately removed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Transforming strings to numeric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re-assign strings within the sex feature to numeric values:\n",
    "\n",
    "# This can be achieved most easily using the .map() function\n",
    "# We can then use a dictionary to map male and female keys to value pairs which we will use inplace of the keys:\n",
    "\n",
    "titanic['sex'] = titanic['sex'].map({'male': 1, 'female': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "üîç _For more information on the .map() function, see [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html)_\n",
    "_____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check your data - has the data updated appropriately?:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Prepare feature and response variables**\n",
    "- As a final data preparation step, we need to comfirm what our **feature** varaibles are and what our **response** variable is.\n",
    "    + Features = the things (columns) we want to use to predict our outcome (i.e. age, gender, class, fare...)\n",
    "    + Response = the outcome we want to predict (i.e. survival or not)\n",
    "    \n",
    "- To do this, we need to seperate out our data into and `X` and `y` datatable, where:\n",
    "    + `X` contains our feature variables\n",
    "    + `y` contains out response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe containing all feature variables you are interested in using as predictors within your model\n",
    "\n",
    "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your features - all okay - contain everything you expect?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create y variable containing your outcome of interest:\n",
    "y = titanic['survived']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your newly created y variable - all okay - contain everything you expect?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "### (1a) Applying Machine Learning Algorithms üé∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we have suitably prepared our data, we are ready to do some machine learning!\n",
    "\n",
    "\n",
    "- As mentioned at the start, there are **many machine learning algorithms** to choose from, each with differing approaches to fitting feature data to response variables. \n",
    "- You need to explictly tell python which algorithm you wish to apply to your data - and depending on which algorithm you use, you may get very different results!\n",
    "- Machine learning can get **complicated** quickly. We will merely touch the surface here to introduce you to the general idea, but do explore wider possibilites - there's some **super cool** stuff out there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________\n",
    "### Machine Learning using Random Forest üå≤\n",
    "\n",
    "- Random Forest one available machine learning algorithm which makes use of **decision trees** to classify outcomes on the basis of features.\n",
    "\n",
    "\n",
    "- This is a **supervised** machine learning technique - we **give the algorithm information about our outcome** (survived / not), and features associated with this outcome, and the algorithm will decide, based on the strength of relationship between these things, how best to seperate the data and classify future cases.\n",
    "\n",
    "\n",
    "- It is a method which can be used for both continous or categorical variables - here we will use it for categorical.\n",
    "\n",
    "\n",
    "- **Very briefly** the techniques works by:\n",
    "    + Splitting the data into two or more sub-populations, based on particular features within the data.\n",
    "    + Identifying the \"most important\" splitter - prioritising this, and then splitting again, again considering all features to best split the next level.\n",
    "    + Doing this many many many many many times across many different trees and finding the most predictive set of \"classifiers\" (features) which best predict the outcome of interest\n",
    "    + Using the most predictive set of tree structures to then make a decision about future outcomes based on feature variables passed into the model.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/random_forest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "üîçMore information on Random Forest can be found [here](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)\n",
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <font color=midnightblue> üíª 4 key workflow steps for ML in python: </font>\n",
    "\n",
    "**1. Import**\n",
    "\n",
    "- Import the machine learning model you wish to use from an appropriate python library.\n",
    "\n",
    "**2. Instantiate**\n",
    "\n",
    "- Initiate the model and save it as an object\n",
    "\n",
    "**3. Fit**\n",
    "\n",
    "- Fit the X features to your y variable using your machine learning model.\n",
    "\n",
    "**4. Predict**\n",
    "\n",
    "- Pass through certain features to your fitted model and see whether the model would predict an individual with those features to survive the titanic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Import you model - in this case we are using RandomForestClassifier, available within the sklearn package:\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Instantiate the model and assign it to object \"random_forest\"\n",
    "random_forest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Fit the data within your model using the .fit() function:\n",
    "random_forest.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...3b: We can check how well our model fit our data using the .score() function as below.\n",
    "## Run the command - what do you think the output is telling us here?\n",
    "random_forest.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To **predict**, let's relook at our feature variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we have 6 feature variables included in our model.\n",
    "- Now that we have fit our model using information from these variables, we can pass through \"new\" values for each of these features into our trained model, which would represent a new observation (which we did not use to train our model), to see whether this \"new observation\" (i.e. representing a new hypothetical passenger) would survive the sinking titanic!\n",
    "- To do this, you pass through a list, containing (in this case) 6 values, which represent each of your feature variables.\n",
    "**For example:** If I wanted to predict the outcome for: a first class, male passenger, aged 32, with no siblings or children onboard who payed a fee of ¬£151, I would pass through the following list of numbers:\n",
    "\n",
    "`[1,0,32.0,0,0,151]`\n",
    "\n",
    "**Where:**\n",
    "- 1st class passenger= `pclass=1`\n",
    "- Male = `sex=1`\n",
    "- 32 years old= `age=32`\n",
    "- 0 siblings= `sibsp=0`\n",
    "- 0 children= `parch=0`\n",
    "- a fee of 151= `fare=151`\n",
    "\n",
    "\n",
    "_Notice how the values passed through in the list must match the order in which variables are specified within the columns_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demonstrating this in practice:\n",
    "random_forest.predict([[1,1,32.0,0,0,151]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>üí¨ Discussion </font>\n",
    "\n",
    "* what do you think the value returned here means? ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "\n",
    "**Predictive probabilities**\n",
    "- Rather than returning a binary \"survived\"/\"did not survive\" outcome, you can also return predictive probabilties using the `.predict_proba()` function.\n",
    "- This tells you, based on the trained model, the probability of surviving verses the probability of dying, given the features you pass into your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demonstrating this in practice:\n",
    "random_forest.predict_proba([[1,1,32.0,0,0,151]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>üí¨ Discussion 1</font>\n",
    "\n",
    "* What inferences can we make based on this output? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>üí¨ Discussion 2</font>\n",
    "\n",
    "* Which output do you think is more useful - `.predict()` or `.predict_proba()`? ü§î\n",
    "* Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "**Feature Importance**\n",
    "\n",
    "- Random forest splits data on the basis of features it thinks are the most important in determining the outcome of interest.\n",
    "- Some features will be treated with higher importance than others - that is, they have more \"predictive power\" in determining the outcome.\n",
    "- After training your model, you can run the `.feature_importances_` command to confirm which features have been assigned higher predictive importance in the trained model than others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running feature_importances_ command\n",
    "random_forest.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The standard output is a little hard to interpret though - especially when you have MANY features. A list of numbers are returned which correspond to the column features passed through the model, but you don't want to hunt through with the naked eye to try and figure out which are more important than others. This is labour intensive and prone to error üò©\n",
    "\n",
    "\n",
    "- Instead, we can use pandas üêº, to pass the information into a dataframe, and then sort the values using a `.sort_values()` function to return our features in order of predictive importance üôå "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below allows you to sort your X feature variables in order of importance:\n",
    "\n",
    "importances = pd.DataFrame({'feature':X.columns,\n",
    "                            'importance': random_forest.feature_importances_})\n",
    "importances = importances.sort_values('importance',ascending=False).set_index('feature')\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>üí¨ Discussion </font>\n",
    "\n",
    "* Which feature(s) did the model assign higher predictive importance?\n",
    "* What do you think this means in terms of predicting outcomes for new cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "* You can also use data visualisation to help visualise the relative importance of features within the model also.\n",
    "\n",
    "* Again, particularly useful if you have a particularly large set of features (which is common in real world machine learning scenarios), and you just want to eyeball the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importances.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "üîçsee other options for plotting feature importance [here](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)\n",
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>‚å® Task </font>\n",
    "- Create a new set of X feature variables, this time only containing the following features:\n",
    "    + sex\n",
    "    + age\n",
    "    + sibsp\n",
    "    \n",
    "    \n",
    "- Train your random forest model on these new, reduced set of feature variables.\n",
    "\n",
    "\n",
    "- Use information about yourself (i.e. your own sex, your age, and how many siblings you have) to predict whether you would survive the sinking titanic?!\n",
    "\n",
    "    - _Imagine that if you have siblings (in real life that is...), that you take all of these siblings onto the titanic with you._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________\n",
    "### Machine Learning using K-Nearest Neighbours üë•\n",
    "\n",
    "- An alternative algorithm to Random Forest is the k-nearest-neighbors (KNN) algorithm.\n",
    "\n",
    "\n",
    "- KNN, like Random Forest, is a **supervised** machine learning method.\n",
    "\n",
    "\n",
    "- **(Again, VERY briefly)**, KNN works on the assumption that similar things exist in close proximity. In other words, similar things are near to each other in multidimensional space.\n",
    "\n",
    "\n",
    "- In other words, if I have charactoristics similar (or \"closer\") to someone with a particular outcome, then I too am more likely to possess that outcome.\n",
    "\n",
    "\n",
    "- If we have a new observation, we can therefore use information from a trained model to label our new observation based on how its \"nearest neighbours\" (most similar observations) in the trained data are labelled.\n",
    "\n",
    "\n",
    "- We can set 'k' to any number, and the algorithm will check the 'k' nearest neighbours to our new observation to classify it. For example, if we set 'k' to 10, the algorithm would check the 10 nearest neighbours to our new observation - and if a higher % of those 10 points were e.g. \"survive\", our new observation would too be classified as \"survive\", whereas if a higher % of those 10 points were e.g. \"not survive\", then our new observation would too be classified as \"not survive\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/knn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "üîç More information on KNN can be found [here](https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/) and [here](https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "#### <font color=midnightblue> üíª ALWAYS REMEMBER THE 4 KEY WORKFLOW STEPS FOR ML IN PYTHON: </font>\n",
    "\n",
    "1. Import\n",
    "\n",
    "2. Instantiate\n",
    "\n",
    "3. Fit\n",
    "\n",
    "4. Predict\n",
    "\n",
    "Let's give KNN a go below using the feature and response variables already generated üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Import:\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Initiate: \n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Fit\n",
    "knn.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3b. Assess how well the model has fit the data - how does this compare to how well random forest fit the data? \n",
    "knn.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Predict - do we get the same prediction for the same new observation as we did for random forest?\n",
    "\n",
    "knn.predict([[1,1,32.0,0,0,151]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4b. Look at the predictive probability - how does this compare to the predictive probability of random forest?\n",
    "knn.predict_proba([[1,1,32.0,0,0,151]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**NOTE:** Unlike random forest - KNN Algorithm does **not** provide any prediction for feature importance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>üí¨ Discussion </font>\n",
    "\n",
    "* Based on the results from RF and KNN for our same new observation - which model do we think did a better job (if any) at predicting our outcome based on our feature variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________\n",
    "## üìΩ Closing Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of what's been covered \n",
    "We have covered **A LOT** of ground in the last 4 weeks! üò∞\n",
    "\n",
    "Some of the key things we've explored have been:\n",
    "- Object assignment\n",
    "- Basic arithmetic \n",
    "- Core data types (integers, lists, dictionaries etc)\n",
    "- Slicing\n",
    "- Loops\n",
    "- Libraries üêºüê¨üìäüìàüé∞\n",
    "- Pandas\n",
    "- Data mining\n",
    "- Data merging\n",
    "- Basic statistics\n",
    "- Charting\n",
    "- Basic machine learning\n",
    "\n",
    "Make sure you are familiar with each of these areas for the assignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "## The assignment üìù\n",
    "\n",
    "- Will be **released the last week of March**, after the second rotation **(due 10:30 on Tuesday 14th April)**.\n",
    "\n",
    "\n",
    "- **Open book**: Use google! Use the documentation! Use your notes! Get in the habit of using the help around you!\n",
    "\n",
    "\n",
    "- You will be provided with a simulated dataset(s) as part of the assignment. Expect missing data, various datatypes, and ambiguity - much like you would expect in a real research setting. Be prepared to be able to use the techniques we have learnt to deal with and make sense of this data in order to make some sensible inferences using statistics, charts and models.\n",
    "\n",
    "\n",
    "- Work is expected to be submitted in the form of a Jupyter Notebook. Make sure you're familiar with creating new notebooks, adding codechunks etc and accessing the document from your local folder space.\n",
    "\n",
    "\n",
    "* Marking is equally weighted across 4 core areas:\n",
    "    + **Code quality and comprehension** - CLEAR code which is easy to interpret. Errors should not appear in your output.  \n",
    " \n",
    "    + **Understanding and execution of charting components** - This includes application with core statistics covered within the course. \n",
    "    \n",
    "    + **Understanding and execution of basic machine learning** - You will be expected to show knowledge of at least _2_ maching learning techniques.\n",
    "    \n",
    "    + **Code reasoning and interpretation of output** - Understanding what you have done is as important as knowing how to run the code.\n",
    " \n",
    " \n",
    " \n",
    "- A marking rubric is available on KEATS which overviews expectations for each of these areas - familiarise yourselves with it!\n",
    "    \n",
    "    \n",
    "### ‚úÖ **For the best marks**\n",
    "- Show that you have **engaged with the wider documentation** and libraries available online. Show us something that wasn't explicitly taught within the workshops (e.g. extra parameters within charting components, other machine learning algorithms available within python). Sticking to the taught content, if done well, will get you good marks, but showing us **more** will get you the best marks.\n",
    "\n",
    "\n",
    "- Demonstrate that you have a **thorough understanding** of what you have done and why. What are the implications of the output? Why might what you have done be useful in a real world research setting?\n",
    "\n",
    "\n",
    "- **Simplicity over complexity** - always try and reach your output using as few a steps as possible (this will make your life easier down the line if you continue to code also!).\n",
    "\n",
    "\n",
    "\n",
    "### üí° **Some extra tips** \n",
    "\n",
    "- Comments comments comments!! You will be awarded marks for commenting on your code chunks (#). It's an easy place to pick up extra marks!\n",
    "\n",
    "\n",
    "- If you find your output is continually failing and you can't run a piece of code - keep the code produced in your code-chunk, but comment it out with a # so that it's not evaluated by python. That way, we can still look to assign marks based on your code and can provide feedback as to why the code may have errored (and you won't \"lose\" marks, so you have nothing to lose by keeping the code in!).\n",
    "\n",
    "\n",
    "- Errors are most frequently due to:\n",
    "    + Forgetting to load a required library\n",
    "    + Forgetting to run an earlier code chunk which the current code chunk relies on\n",
    "    + Forgetting to put a : in things like loops.\n",
    "    + Forgetting to close off brackets\n",
    "    + Forgetting that python is case sensitive\n",
    "    + Missing a comma somewhere (this happens to me on a daily basis...)\n",
    "    + Trying to call the wrong data type (remember how casting can help here)\n",
    "    + Pointing to the wrong index (remember they start at 0)\n",
    "\n",
    "üëÜüëÜüëÜüëÜ**Make sure you check all of these before you give up on your code!**\n",
    "\n",
    "<br>\n",
    "\n",
    "- Make use of the forum. There are no stupid questions. If you are struggling with something, likely others are too, and it's likely that I haven't explained it well enough, so don't be afraid to ask!\n",
    "\n",
    "\n",
    "- Remember: **GOOGLE IS YOUR BEST FREIEND!!**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color=blue> **I will post some extra learning / revision material in the weeks between now and the assignment. KEEP PRACTICING!** </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ‚ùó **Finally:**\n",
    "\n",
    "\n",
    "### **Have fun with it!!** üéâ \n",
    "This is your opportunity to really explore what python has to offer and to be creative with your code! Push the boundaries, make mistakes, and learn what works and what doesn't. The possibilites are limited only by how far you wish to take your code.\n",
    "\n",
    "üôã"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
